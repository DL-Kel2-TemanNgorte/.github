<!-- # Kelompok 2 - Teman Ngorte. -->

<h1 align="center" width="100%">
    <a id="anggota-kelompok"></a>
    <b>Kelompok 2 - Teman Ngorte.</b> <br>
    â€»â€»â€»
</h1>

# **Anggota**
- ### Vidya Chandradev - 1905551067 ([catgoesmeow14](https://github.com/catgoesmeow14))
- ### Anak Agung Istri Prabhaisvari Sadhaka - 1905551044 ([prbisv](https://github.com/prbisv))
- ### I Putu Bayu Adhya Wiratama - 1905551059 ([TamaWira](https://github.com/TamaWira))
- ### I Gusti Made Diva Widia Wiartha - 1905551081 ([divawidia](https://github.com/divawidia))
- ### Ni Luh Gede Midya Frangginie - 1905551110 ([Midyafn](https://github.com/Midyafn)) 
  
---

<a id="daftar-isi"></a>
# **Daftar Isi**
<details>
<summary><b>Klik untuk Lihat Detailâ¤µï¸</b></summary>
<!-- MarkdownTOC -->

1. [Anggota Kelompok 2](#anggota-kelompok)
2. [Daftar Isi](#daftar-isi)
3. [Deskripsi Aplikasi](#deskripsi-aplikasi)
4. [Tutorial](#tutorial)
5. [Arsitektur Aplikasi](#arsitektur-aplikasi)
6. [Dataset](#dataset)
7. [Algoritma Deep Learning](#algoritma-deep-learning)
    1. [LSTM](#lstm)
       1. [Analisis Model LSTM](#analisis-model-lstm)
       2. [Training dan Testing LSTM](#training-testing-lstm)
    2. [biLSTM](#bilstm)
       1. [Analisis Model biLSTM](#analisis-model-bilstm)
       2. [Training dan Testing biLSTM](#training-testing-bilstm)
    3. [BERT](#bert)
       1. [Analisis Model BERT](#analisis-model-bert)
       2. [Training dan Testing BERT](#training-testing-bert)
    4. [Perbandingan Masing-masing Model](#perbandingan-model)
    5. [Alasan Model yang Digunakan Paling Unggul](#alasan-model-unggul)

<!-- /MarkdownTOC -->
</details>

---

<a id="deskripsi-aplikasi"></a>
# **Deskripsi Aplikasi**  [ğŸ”](#daftar-isi)

#### Sekitar <ins>**19 juta**</ins> orang yang berumur >15 tahun menderita mental illness berdasarkan data Riskesdas 2018. Litbangkes juga menyebutkan bahwa angka kasus bunuh diri setiap tahunnya hampir mencapai <ins>**2 ribu**</ins> orang. Selain itu, situs Integrasi Layanan Rehabilitasi Sosial menyatakan Indonesia <ins>**hanya memiliki 48 rumah sakit jiwa**</ins>, dengan 32 milik negara dan 16 swasta. Bahkan **8** provinsi belum memiliki rumah sakit jiwa. Tidak hanya itu, jumlah tenaga kesehatan jiwa profesional yang tersedia hanya berkisar <ins>**1.053 orang**</ins> berdasarkan penelitian terbaru. 

<p align="center" width="100%">
   â—†â—†â—†
</p>

### Penjabaran tersebut menunjukkan bahwa . . .

<h2 align="center" width="100%">
   <b>â—MENTAL  HEALTH  DI  INDONESIA MASIH CUKUP MENGKHAWATIRKANâ—</b><br>
   . . .
</h2>


 ### Untuk itulah kami menghadirkan **solusi**ğŸ’¡ berupa sebuah aplikasi yang dapat diakses **kapan saja**ğŸ•‘ğŸ•ğŸ•’, **di mana saja**ğŸ™ï¸ğŸ¡ğŸ«, dan **oleh siapa saja**ğŸ§œğŸ»â€â™€ï¸ğŸ§‘ğŸ»â€ğŸ’»ğŸ•µğŸ»ğŸ«…ğŸ» **SECARA GRATIS**ğŸ’¸

<h3 align="center" width="100%">
    ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»ğŸ‘‡ğŸ»
</h3>

## **Teman Ngorte.**

<p align="center" width="100%">
    <img width="20%" src="https://i.ibb.co/FstxsVK/teman-ngorte-logo-01.png"> 
</p>

[Teman Ngorte](https://chatbot-app-three.vercel.app/) adalah aplikasi berbasis website sebagai media untuk mengekspresikan perasaan penggunanya agar dapat menghilangkan stres melalui fitur chatbot. Aplikasi ini dapat membantu Anda untuk mengungkapkan perasaan Anda, menghilangkan stres, dan membantu Anda untuk lebih sehat secara mental. Aplikasi ini memiliki User Interface dan User Experience yang baik sehingga menenangkan hati saat melihatnya dan mudah untuk digunakan.

### <ins>**MENGAPA KAMI MEMBUAT TEMAN  NGORTE?** </ins> 

<p align="center" width="100%">
    <img width="67%" src="https://i.ibb.co/9rcBRG7/Teman-Ngorte-Kelompok-2-cropped.png"> 
</p>

---

## **Website Teman Ngorte** â¡ï¸ **[Teman Ngorte](https://chatbot-app-three.vercel.app/)** [ğŸ”](#daftar-isi)

## **Login**
> #### Username : `pencurhat`  
> #### Password : `curhatdong`

---

<a id="tutorial"></a>
# **Tutorial** [ğŸ”](#daftar-isi)


<details>
<summary><b>Klik untuk Lihat Detailâ¤µï¸</b></summary>

#### Projek ini telah dihosting dengan menggunakan **Vercel App**

> **Vercel** merupakan layanan platform cloud yang memungkinkan pengembang menghosting situs web dan layanan web yang diterapkan secara instan, diskalakan secara otomatis, dan bersifat *serverless* dimana pengembang tidak perlu untuk selalu mengawasi dan mengelola server. 

<p align="center" width="100%">
    <img width="80%" src="https://miro.medium.com/max/1400/1*pJdLvOAPgVTfESZlSiCTwQ.webp"> 
</p>

---

#### Untuk menjalankan aplikasi ini, dapat langsung mengakses link berikut

**[Website Teman Ngorte](https://chatbot-app-three.vercel.app/)**

### Cara Menggunakan Aplikasi
Aplikasi ini dapat digunakan dengan **2 cara login** yaitu sebagai berikut

#### 1. Login dapat dilakukan dengan menggunakan Username dan Password

###### Step 1 : Login
Masukkan Username dan Password berikut

> Username : `pencurhat`  
Password : `curhatdong`

![test login](https://user-images.githubusercontent.com/80409196/208438023-3bac8870-89dd-43e9-b416-ba4f9fbbc215.png)

Pastikan Username dan Password sesuai, apabila tidak maka akan muncul **alert** yang menandakan Username atau Password salah

- alert **username** salah
![alertusername](https://user-images.githubusercontent.com/80409196/208442008-1daad28e-f63c-4039-9140-661214856cb2.png)


- alert **password** salah
![alertpass](https://user-images.githubusercontent.com/80409196/208442026-dd7bf761-8459-4c2b-8c07-929bae71782e.png)


###### Step 2 : 
Apabila kombinasi Username dan Password sudah sesuai, klik Pada tombol Login
![test login2](https://user-images.githubusercontent.com/80409196/208438176-cc6e56e5-4dc1-400f-bbec-e4300ee3f8f3.png)

###### Step 3 : 
Klik Pada Tombol "Yuk Curhat!"
![yukcurhat](https://user-images.githubusercontent.com/80409196/208438298-63e6d08c-d5a2-4763-9754-1d2b9c910bd8.png)

###### Step 4 : 
Sampaikan keluh kesahmu pada TimpalBot! TimpalBot akan menjadi teman curhatmu!
![chat](https://user-images.githubusercontent.com/80409196/208438371-13137646-9b63-42ba-9a56-7a0d02452b60.png)

###### Step 5 : 
Untuk kembali atau menutup bubble chat, dapat dilakukan dengan klik pada tombol **back**
![back2](https://user-images.githubusercontent.com/80409196/208438451-e1dbc417-52b4-4acc-9915-766822c8f7d6.png)

###### Step 6 : 
Untuk keluar dari aplikasi, dapat dilakukan dengan klik pada tombol **logout**
![logoutweb3](https://user-images.githubusercontent.com/80409196/208438521-643ba0df-1f4e-48ed-bfcd-6d44eb20feac.png)

###### Step 7 : 
Untuk konfirmasi logout, dapat dilakukan dengan klik **OK** pada pop up konfirmasi logout
![notif logout2](https://user-images.githubusercontent.com/80409196/208438706-d46ab361-9dd4-46ab-8f20-b4a31f76607e.png)

#### 2. Menggunakan **Login as Guest**
Untuk menggunakan Login as Guest, pengguna **tidak perlu** memasukkan **username dan password**

###### Step 1 : 
Klik Pada Tombol Login as Guest
![loginguest](https://user-images.githubusercontent.com/80409196/208438979-8fa47924-92ca-44f4-ade1-b944d3d57b34.png)

###### Step 2 : 
Klik Pada Tombol "Yuk Curhat!"
![welcomeguest](https://user-images.githubusercontent.com/80409196/208439334-12eb3602-5317-4451-a7ce-48913a0a8f30.png)

###### Step 3 : 
Sampaikan keluh kesahmu pada TimpalBot! TimpalBot akan menjadi teman curhatmu!
![chat](https://user-images.githubusercontent.com/80409196/208438371-13137646-9b63-42ba-9a56-7a0d02452b60.png)

###### Step 4 : 
Untuk kembali atau menutup bubble chat, dapat dilakukan dengan klik pada tombol **back**
![back2](https://user-images.githubusercontent.com/80409196/208438451-e1dbc417-52b4-4acc-9915-766822c8f7d6.png)

###### Step 5 : 
Untuk keluar dari aplikasi, dapat dilakukan dengan klik pada tombol **logout**
![backguest2](https://user-images.githubusercontent.com/80409196/208440411-70f801fb-4330-4e5c-9fc1-f1c15e409132.png)

###### Step 6 : 
Untuk konfirmasi logout, dapat dilakukan dengan klik **OK** pada pop up konfirmasi logout
![logoutguest2](https://user-images.githubusercontent.com/80409196/208440696-4991064c-3104-46cc-a536-f64f2a20471d.png)


</details>

---

<a id="arsitektur-aplikasi"></a>
# **Arsitektur Aplikasi** [ğŸ”](#daftar-isi)

<p align="center" width="100%">
    <img src="https://user-images.githubusercontent.com/62200309/208637731-815af47e-0080-44d6-a7cc-32b6f845ff9a.png"> 
</p>

Arsitektur dari chatbot Teman Ngorte terdiri dari 2 bagian yaitu Front End dan Backend. Pada bagian Fron End menggunakan framework React.js dan juga tailwind CSS untuk membuat user interface dari aplikasi chatbot. Bagian Front End dari chatbot ini kemudian dideploy di platform Vercel agar bisa diakses melalui internet. Pada bagian Back End menggunakan framework Flask dan bahasa pemrograman python untuk memproses segala logic dibelakang layar aplikasi chatbot teman ngorte dan membuat API yang digunakan untuk komunikasi data antara Front End dan Back End. Database yang digunakan pada aplikasi chatbot teman ngorte yaitu MySQL Database yang dideploy di pltform Railway. Aplikasi chatbot teman ngorte ini menggunakan model deep learning BiLSTM yang di buat menggunakan library Tensorflow, dimana model ini digunakan untuk memproses respons yang diberikan oleh chatbot terhadap chat atau pertanyaan dari pengguna.  Bagian Back End ini dideploy menggunakan platform cloud dari Google yang Bernama Google Cloud Platform dengan layanan Cloud Run dan Docker untuk mendeploy aplikasi Back End dari chatbot Temen Ngorte. Komunikasi antara bagian Front End dengan Back End menggunakan JSON API, dimana Bagian Front End mengirim HTTP Request ke bagian Back End, lalu pada  bagian Back End akan memproses HTTP Request tersebut dan mengirim hasil prosesnya dalam bentuk JSON API ke bagian Front End.

---

<a id="dataset"></a>
# **Dataset** [ğŸ”](#daftar-isi)

Dataset yang digunakan bersumber dari berbagai *website* antara lain sebagai berikut.
- Kaggle ([https://www.kaggle.com/datasets/narendrageek/mental-health-faq-for-chatbot](https://www.kaggle.com/datasets/narendrageek/mental-health-faq-for-chatbot))
- Alodokter ([https://www.alodokter.com/komunitas/topic-tag/kesehatan-mental](https://www.alodokter.com/komunitas/topic-tag/kesehatan-mental))
- Hellosehat ([https://hellosehat.com/community/kesehatan-mental/](https://hellosehat.com/community/kesehatan-mental/))
- SehatQ ([https://www.sehatq.com/forum/psikologi?](https://www.sehatq.com/forum/psikologi?))
- GitHub ([https://raw.githubusercontent.com/bhumikabhatia/mental-health-chatbot-Miklal/main/model/20200325_counsel_chat.csv](https://raw.githubusercontent.com/bhumikabhatia/mental-health-chatbot-Miklal/main/model/20200325_counsel_chat.csv))

Data dari sumber-sumber tersebut dikumpulkan menjadi satu *file* berformat .csv dengan 3 kolom yakni pertanyaan, jawaban, dan label. Data yang didapatkan dari Kaggle dan GitHub telah memiliki label, sehingga data yang berasal dari *website* lainnya dilabeli secara manual. Data yang berbahasa Inggris juga diterjemahkan secara manual. Total data yang diperoleh berjumlah 1981 sampel dengan 18 kelas yaitu salam, depresi, keluarga, hubungan, kecemasan, konseling_fundamental, harga_diri, trauma, cari_bantuan, perubahan_perilaku, definisi, penyalahgunaan_zat, manajemen_emosi, tips_pengobatan, diagnosis_dini, peningkatan_pola_tidur, profesional_yang_benar, dan bye_thanks. Adapun persebaran data pada masing-masing labelnya sebagai berikut.

| No.  | Label                    | Jumlah      |
|------|--------------------------|-------------|
| 1.   | salam                    | 11          |
| 2.   | depresi                  | 359         |
| 3.   | keluarga                 | 334         |
| 4.   | hubungan                 | 328         |
| 5.   | kecemasan                | 249         |
| 6.   | konseling_fundamental    | 240         |
| 7.   | harga_diri               | 83          |
| 8.   | trauma                   | 66          |
| 9.   | cari_bantuan             | 58          |
| 10.  | perubahan_perilaku       | 51          |
| 11.  | definisi                 | 50          |
| 12.  | penyalahgunaan_zat       | 40          |
| 13.  | manajemen_emosi          | 38          |
| 14.  | tips_pengobatan          | 23          |
| 15.  | diagnosis_dini           | 22          |
| 16.  | peningkatan_pola_tidur   | 10          |
| 17.  | profesional_yang_benar   | 9           |
| 18.  | bye_thanks               | 10          |


---

<a id="algoritma-deep-learning"></a>
# **Algoritma Deep Learning** 
<a id="lstm"></a>
## **LSTM** [ğŸ”](#daftar-isi)

<details>
<summary><b>Klik untuk Lihat Detailâ¤µï¸</b></summary>

### Penjelasan Algoritma LSTM
Algoritma LSTM atau *Long Short Term Memory* merupakan bentuk pengembangan dari *Reccurent Neural Network* yang dapat melakukan prediksi berdasarkan data yang tersimpan dalam jangka waktu panjang sekaligus dapat menghapus data yang sudah tidak perlu digunakan.

![Alt text](https://miro.medium.com/max/828/1*7cMfenu76BZCzdKWCfBABA.webp)

Berdasarkan penelitian yang dilakukan oleh **Hochreiter dan Schmidhuber** di tahun 1997 yang kemudian menjadi cikal bakal arsitektur LSTM modern, LSTM terbukti dapat menyelesaikan permasalahan yang cukup kompleks dan belum atau kurang dapat diselesaikan oleh algoritma *reccurent* lainnya.

#### Gates Pada LSTM 
Terdapat tiga bagian gates yang hanya digunakan pada algoritma *Long Short-Term Memory*, dimana masing-masing memiliki fungsinya tersendiri. Penjelasan mengenai gates tersebut adalah sebagai berikut. 

###### Forget Gate
Bagian pertama yang dinamakan forget gates merupakan bagian yang akan menentukan apakah suatu informasi akan digunakan kembali atau diingat atau dilupakan.

![Alt text](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)

###### Input Gate
Bagian kedua yaitu input gate yang berfungsi untuk mempelajari informasi baru yang diinputkan pada bagian ini

![Alt text](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)

###### Output Gate
Bagian ketiga yaitu output gate, yaitu bagian yang akan memberikan informasi yang telah diperbarui pada timestamp saat itu ke timestamp berikutnya

![Alt text](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png)

#### LSTM untuk Text Classification
  Terdapat banyak algoritma lainnya yang dapat digunakan untuk klasifikasi text seperti SVM atau Neural Network pada umumnya. Akan tetapi sebagian besar algoritma tersebut tidak memberikan hasil yang baik jika dibandingkan dengan LSTM. Hal tersebut karena LSTM mampu untuk mengingat informasi penting dengan efektif.

  Dengan menggunakan LSTM kita dapat menggunakan beberapa string kata untuk mengetahui kelas yang menjadi tempatnya. Hal tersebut sangat membantu dalam proses pengerjaan *natural language processing*. Jika menggunakan lapisan penyematan dan penyandian yang sesuai di LSTM, model akan dapat mengetahui arti sebenarnya dari *string* input dan akan memberikan kelas *output* yang paling akurat. 

</details>

<a id="analisis-model-lstm"></a>
### **Analisis Model LSTM** [ğŸ”](#daftar-isi)

<details>
<summary><b>Klik untuk Lihat Detailâ¤µï¸</b></summary>

##### **Tahapan Penelitian**

Pada implementasi model LSTM ini, **terdapat dua percobaan**. Perbedaan kedua percobaan ini terdapat pada **jumlah kelas**, dan **jumlah epoch** pada proses training.

Percobaan pertama hanya diambil 16 kelas dari total 18 kelas, dengan pengecualian terhadap kelas `salam` dan `bye_thanks`. Percobaan kedua menggunakan keseluruhan kelas.

Jumlah epoch pada proses training percobaan pertama adalah **120 epoch**, dan percobaan kedua adalah **150 epoch**.

##### **Alur Kerja**

1. **Text Preprocessing**, yang dilakukan dengan mengubah seluruh teks ke huruf kecil dan melakukan operasi regex untuk menghilangkan karakter atau simbol yang tidak diinginkan.
2. **Split dan shuffle dataset**, untuk memisahkan dataset training dan uji untuk mengukur performa akurasi model terhadap data yang berbeda.
3. **Tokenisasi**, untuk mengubah teks ke dalam bentuk representasi nilai angka yang dapat dimengerti oleh model untuk proses training.
4. **Proses Training**, dimana model LSTM dilatih dengan dataset agar dapat melakukan klasifikasi teks chat dengan baik.
5. **Evaluasi**, untuk melihat ukuran hasil pelatihan berdasarkan nilai akurasi dan loss dari proses training menggunakan dataset training dan uji.

##### **Arsitektur Model**

Model diimplementasikan menggunakan framework TensorFlow yang disimpan dalam format HDF5. Visualisasi arsitektur model yang digunakan dapat dilihat pada gambar berikut.

- Arsitektur Percobaan Pertama
    ![Alt text](https://i.ibb.co/xMwkL35/chatbot-model-lstm-withval-databaru-h5-2.jpg "Arsitektur LSTM pertama")  
- Arsitektur Percobaan Kedua
    ![Alt text](https://i.ibb.co/8BgDNMB/chatbot-model-lstm-withval-datafix72-h5.jpg, "Arsitektur LSTM kedua")

Perbedaan arsitektur dari kedua percobaan hanya terletak pada dense layer keluaran, dimana percobaan pertama berjumlah 16 unit neuron dan percobaan kedua berjumlah 18 unit neuron. Jumlah unit ini disesuaikan dengan jumlah kelas keluaran model.

###### **Implementasi Model**

Implementasi model menggunakan TensorFlow dilakukan secara sekuensial, dengan menggabungkan beberapa layer secara berurutan. Parameter yang diatur dalam implementasi secara lebih jelas dapat dilihat pada source code dalam blok kode berikut.

```python
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]),
    tf.keras.layers.SpatialDropout1D(0.2),
    tf.keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),
    tf.keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(16, activation='softmax') # Output percobaan pertama 16 kelas, percobaan kedua 18 kelas
])
```

###### **Penjelasan Arsitektur**

Model diimplementasikan dengan total 6 lapisan secara berurutan, yang antara lain adalah sebagai berikut.

- Lapisan pertama digunakan lapisan **Embedding** dengan ukuran vocabulary 3000 kata dan panjang input 150 data representasi token untuk satu kalimat sesuai ukuran pada data (X_train.shape[1]). Ukuran keluaran embedding adalah 150 x 150 (150 pertama merupakan nilai yang didefinisikan sendiri, 150 berikutnya merupakan panjang input).
- **SpatialDropout1D** digunakan pada lapisan berikutnya untuk mencegah overvitting dengan **menghilangkan 20 persen fitur**. **SpatialDropout1D** sama seperti Dropout biasa, namun **SpatialDropout1D** menghilangkan seluruh feature map 1 dimensi ketimbang elemen individual.
- Diikuti dengan lapisan **LSTM** dengan jumlah unit 64, nilai droput 0.2 dan recurrent dropout 0.2 untuk mencegah overvitting. Diberikan parameter **return_sequences=True** agar model mengembalikan output terakhir dalam sequence output **LSTM**. Fungsi aktivasi recurrent yang digunakan adalah sigmoid dan fungsi aktivasi keluaran yang digunakan adalah TanH.
- Dilanjutkan dengan lapisan **LSTM** dengan parameter yang sama, namun tanpa **return_sequences=True**, sehingga layer mengembalikan seluruh output sequence.
- **Dense** layer digunakan sebagai lapisan fully connected neural network, dengan jumlah neuron sebanyak 64 dan fungsi aktivasi relu.
- Digunakan **Dense** layer pada lapisan keluaran untuk menghasilkan output 16 kelas yang diklasifikasi. Fungsi aktivasi yang digunakan adalah **softmax** agar dapat memberikan keluaran probabilitas masing-masing kelas.

##### **Evaluasi Model**

Model dilatih dengan dataset yang dibagi menjadi data training dan uji, dengan data **training 80%** dan **data uji 20%**. Proses training dilakukan dengan **ukuran batch 128**.

Jumlah epoch pada percobaan pertama adalah **120 epoch**, dan percobaan kedua adalah **150 epoch**.

Fungsi loss yang digunakan adalah `categorical_crossentropy` karena label yang digunakan **one-hot encoded** dengan keluaran fungsi aktivasi softmax pada lapisan keluaran model yang menghasilkan probabilitas untuk masing-masing label.

Optimizer yang digunakan adalah `Adam` karena merupakan optimizer adaptif yang dapat digunakan pada data sparse dan dapat digunakan tanpa mendefinisikan learning rate.

Metrik keluaran model yang ingin diukur adalah `accuracy`, untuk melihat kesesuaian hasil klasifikasi model.

###### **Grafik Nilai Loss dan Akurasi LSTM**

Evaluasi hasil training model dilakukan dengan melihat nilai loss dan akurasi pada proses training dan validasi dengan data uji. Grafik nilai loss dan akurasi dapat dilihat pada gambar berikut.

- Grafik Percobaan Pertama
    ![Alt text](https://i.ibb.co/YfWXHRT/output-120.jpg "Hasil Evaluasi Model 1")  

    Grafik nilai akurasi menunjukkan bahwa nilai akurasi baik pada proses training maupun validasi mengalami peningkaan secara signifikan. Namun, pada grafik akurasi validasi tidak terjadi peningkatan signifikan ketika mencapai epoch ke-40.

    Grafik nilai loss menunjukkan bahwa nilai loss pada proses training mengalami penurunan signifikan dari 1.0 menjadi 0.15, namun nilai loss validasi mengalami peningkatan signifikan dari 1.5 menjadi 2.73. Hal ini menunjukkan bahwa model belum berhasil melakukan generalisasi sehingga terjadi peningkatan nilai loss ketika berhadapan dengan data uji.

- Grafik Percobaan Kedua
    ![Alt text](https://i.ibb.co/6DydDcv/output2.jpg "Hasil Evaluasi Model 2")  

    Grafik nilai akurasi menunjukkan bahwa nilai akurasi baik pada proses training maupun validasi mengalami peningkaan, namun pada beberapa kurun waktu sempat mengalami stagnansi, dan beberapa lainnya mengalami peningkatan tajam.

    Grafik nilai loss menunjukkan bahwa nilai loss pada proses training mengalami penurunan signifikan dari 2.75 menjadi 0.8, dan nilai loss validasi dari 2.5 menjadi 1.73. Hal ini juga menunjukkan bahwa model berhasil melakukan pembelajaran dengan baik namun masih belum optimal.

###### **Hasil Evaluasi Model LSTM**

- Hasil Percobaan Pertama
  
    Hasil training pada epoch terakhir (ke-120) didapatkan nilai akurasi training mencapai 92,03% dan akurasi validasi 69,75%. Nilai loss pada proses training adalah sebesar 0,1436 dan loss nilai validasi adalah sebesar 0,6975.

    ![Alt text](https://i.ibb.co/42ph3v3/Screenshot-2022-12-20-at-20-10-37.png "Nilai Akurasi dan Loss Epoch Terakhir Percobaan 1")

- Hasil Percobaan Kedua

    Hasil training pada epoch terakhir (ke-150) didapatkan nilai akurasi training mencapai 72,93% dan akurasi validasi 62,46%. Nilai loss pada proses training adalah sebesar 0,8099 dan loss nilai validasi adalah sebesar 1,7347.

    ![Alt text](https://i.ibb.co/gyjZzY5/Screenshot-2022-12-20-at-20-08-26.png "Nilai Akurasi dan Loss Epoch Terakhir Percobaan 2")

Perbandingan hasil kedua percobaan lebih jelas dapat dilihat pada tabel berikut.

| Metrik              | Hasil Percobaan 1 | Hasil Percobaan 2 |
| :------------------ | ----------------: | ----------------: |
| Training Loss       |            0,1509 |            0,8099 |
| Validation Loss     |            2,7392 |            1,7347 |
| Training Accuracy   |            92,03% |            72,93% |
| Validation Accuracy |            69,75% |            62,46% |

Hasil evaluasi model menunjukkan bahwa **performa model LSTM terbaik didapat dari percobaan pertama**. Akurasi model pada proses training yang dihasilkan cukup tinggi, yaitu mencapai 92 persen, namun akurasi pada proses pengujian menggunakan dataset validasi hanya mencapai 69 persen. Hal ini menunjukkan bahwa model masih belum berhasil melakukan klasifikasi teks dengan baik dan perlu penelitian lebih lanjut.

Hasil percobaan kedua mendapatkan performa akurasi dan loss yang lebih buruk, karena terdapat perbedaan jumlah kelas. Dimana percobaan pertama hanya terdapat 16 kelas, sedangkan percobaan kedua terdapat 18 kelas. Peningkatan jumlah epoch pada percobaan kedua juga memiliki kontribusi yang kurang signifikan terhadap upaya peningkatan performa dari percobaan pertama.

</details>

<a id="training-testing-lstm"></a>
### **Training dan Testing LSTM** [ğŸ”](#daftar-isi)

<details>
<summary><b>Klik untuk Lihat Detailâ¤µï¸</b></summary>

#### LSTM
---
Proses pembuatan model LSTM (*Long-Short Term Memory*) dilakukan dalam 2 tahap, yaitu *training* dan *evaluation*. Berikut merupakan penjelasan masing-masing tahapannya.

##### 1. Training
Pada tahap *training*, beberapa proses yang dilakukan antara lain **persiapan data training**, **pembuatan arsitektur model**, dan **pelatihan model**.

###### 1.1 Persiapan Data Training
Pada tahap ini, seluruh dataset yang digunakan akan dipecah (*split*) menjadi beberapa *subset*, yaitu data *train* (data latih) dan data *validation* (data validasi). Data *train* dan data *validation* dibuat dengan fungsi `train_test_split` dari *library* sklearn. Seluruh *dataset* dipecah menjadi dua *subset*, yaitu *data train* dan *data validation* dengan proporsi 80% dan 20%. Data *train* kemudian digunakan pada tahap *training* dan data *validation* digunakan pada tahap *evaluation*.

###### 1.2 Pembuatan Arsitektur Model
Pada tahap ini, dilakukan pembuatan arsitektur model LSTM dengan metode *sequential* menggunakan *library* TensorFlow Keras. Berikut merupakan arsitektur dari model LSTM yang dibuat.

![model_plot_lstm18](https://user-images.githubusercontent.com/76557114/208538594-0a5d6a4f-e8b3-4b0f-9a2f-cbde9a022deb.png)

Gambar di atas merupakan arsitektur dari model LSTM yang telah dibuat. Terdapat lapisan input yang berukuran 150 kolom. Input tersebut kemudian mengirimkan data pada lapisan ***Embedding*** dengan jumlah *vocabulary* maksimum sebesar 3000 dan dimensi *embedding* berukuran 150. Hasil *embedding* kemudian akan dikirimkan pada lapisan ***SpatialDropout1D***. Setelah itu, data akan dikirimkan kepada lapisan LSTM pertama dengan jumlah unit sebesar 64. Lapisan berikutnya adalah lapisan LSTM kedua (*stacked*) dengan jumlah unit yang sama yaitu 64. Setelah data melalui lapisan LSTM, kemudian akan dikirim pada lapisan ***Dense*** (neuron biasa) dengan jumlah unit sebesar 64 dan *activation function* ReLu. Hasil dari lapisan ini kemudian akan dikirimkan pada lapisan terakhir (*output layer*) dengan total *output* sebesar 18.

###### 1.3 Training Model
Pada tahap ini, data akan dipelajari representasinya oleh model yang telah dibuat. Pada tahap ini, digunakan dua buah *subset* data yaitu data *train* dan data *validation*. Data *train* digunakan untuk pelatihan model, sementara data *validation* digunakan untuk mengukur performa model pada data yang belum pernah dilihat. Berikut merupakan konfigurasi dari tahap pelatihan model LSTM.

| No. | Hyperparameter | Nilai                           |
|-----|----------------|---------------------------------|
| 1.  | Optimizer      | Adam                            |
| 2.  | Loss           | Categorical Crossentropy        |
| 3.  | Epochs         | 150                             |
| 4.  | Batch Size     | 128                             |

Hasil dari proses pelatihan model akan dibahas pada tahap evaluasi.

</details>

---

<a id="bilstm"></a>
## **biLSTM** [ğŸ”](#daftar-isi)

<details>
<summary><b>Klik untuk Lihat Detailâ¤µï¸</b></summary>

### Penjelasan Algoritma BiLSTM
Bidirectional Long Short â€“ Term Memory Neural Network (BiLSTM) merupakan salah satu varian dari Long Short Term Memory yang paling sering digunakan. Input forward dan input backward adalah 2 jenis masukan yang dimasukkan ke dalam arsitektur Bidirectional Long Short Term Memory. Output dari arsitektur ini biasnya digabungkan jadi satu. Dengan layer arsitektur ini, model bisa menekuni data masa lalu (past) dan data masa mendatang (future) untuk setiap sekuen input.

![Alt text](https://data03.123doks.com/thumbv2/123dok/003/825/3825207/28.892.325.612.235.427/gambar-arsitektur-bilstm-sumber-graves-amp-schmidhuber.webp)

Bidirectional LSTM memanfaatkan informasi sebelumnya dan informasi setelahnya dengan memproses data dari dua arah. Forward layer berfungsi untuk merepresentasikan informasi sebelumnya, dan backward layer berfungsi untuk merepresentasikan informasi setelahnya.

![Alt text](https://docplayer.info/docs-images/113/206254265/images/2-1.jpg)

Gambar diatas merupakan arsitektur hidden layer BiLSTM. Setiap hidden layer keluaran unit pada layer bawah dan atas disatukan hingga membentuk nilai fitur yang lebih panjang dari pada LSTM biasa. Karena nilai fitur pada BiLSTM lebih panjang , maka informasi yang akan diproses pada proses selanjutnya yaitu feed forward neural akan mengklasifikasikan dengan lebih detail.

![arsitektur model bilstm](https://user-images.githubusercontent.com/62200309/208637445-1611a5dc-8940-4165-a90a-9a99c171289e.png)

Gambar diatas adalah arsitektur dari BiLSTM yang merupakan gabungan dari dua LSTM arah maju dan arah mundur. Terdapat hidden layer juga yang terhubung dari LSTM maju dan LSTM mundur. 

Dengan adanya hidden layer dua arah ini yang saling berlawanan maka model dapat memahami data dari depan dan belakang, sehingga proses pelatihan akan lebih memahami data pada time series. BiLSTM akan sangat berguna dalam hal pelatihan sekuensial apabila bisa mengakses dari informasi sebelum dan
sesudahnya. Jika LSTM hanya bisa mengakses informasi dari masa lalu saja, tetapi informasi masa mendatang tidak diketahui. BiLSTM bisa menjadi solusi untuk memecahkan masalah tersebut.

</details>

<a id="analisis-model-bilstm"></a>
### **Analisis Model biLSTM** [ğŸ”](#daftar-isi)

<details>
<summary><b>Klik untuk Lihat Detailâ¤µï¸</b></summary>

### Evaluation

Pada tahap ini, dilakukan pengukuran/evaluasi performa model dalam memprediksi data *testing* berdasarkan data latih yang telah digunakan pada tahap pelatihan. Pada tahap ini, metriks yang digunakan antara lain ***Loss*** dan ***accuracy***. Berikut merupakan hasil evaluasi model.
| Metriks  | Training | Validation | Testing |
|----------|----------|------------|---------|
| Loss     | 0.0984   | 2.0704     | 2.33    |
| Accuracy | 0.9424   | 0.7224     | 0.72    |

| ![Grafik Evaluasi BiLSTM](https://i.ibb.co/1z8jnqw/grafik-evaluasi-Bi-LSTM.png) |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|

Hasil dari tahap pelatihan (*training*) model mencapai akurasi sebesar 94% dan *loss* sebesar 0.09. namun performa model secara keseluruhan menurun pada data validasi, dimana *loss* menurun sebesar 2 poin dan *accuracy* menurun sebesar 22%. Pada tahap *testing*, performa yang dihasilkan mirip dengan performa pada validasi. Hal ini mengindikasikan bahwa model BiLSTM mengalami ***overfitting***.

</details>

<a id="training-testing-bilstm"></a>
### **Training dan Testing biLSTM** [ğŸ”](#daftar-isi)

<details>
<summary><b>Klik untuk Lihat Detailâ¤µï¸</b></summary>

#### BiLSTM

Proses pembuatan model BiLSTM (*Bidirectional Long-Short Term Memory*) dilakukan dalam 3 tahap, yaitu *training* dan *testing*. Berikut merupakan penjelasan masing-masing tahap.
##### 1. Training
Pada tahap *training*, beberapa proses yang dilakukan antara lain **persiapan data latih**, **pembuatan arsitektur model**, **pelatihan model**.

###### 1.1 Persiapan Data Latih
Pada proses persiapan data latih, seluruh *dataset* yang digunakan akan dipecah (*split*) menjadi beberapa *subset*, diantaranya data *train* (latih) dan data *testing* (uji). Data *train* dan data *testing* dibuat dengan melakukan *split* terhadap *dataset* dengan menggunakan fungsi `train_test_split` dari *library* sklearn. Seluruh *dataset* dipecah menjadi dua (2) buah *subset*, yaitu *data train* dan *data testing* dengan proporsi 80% dan 20%. Data *train* kemudian digunakan pada tahap *training* dan data *testing* digunakan pada tahap *testing*.

###### 1.2 Pembuatan Arsitektur Model
Pada proses ini, dilakukan pembuatan arsitektur model BiLSTM dengan metode *sequential* menggunakan *library* Keras. Berikut merupakan arsitektur model BiLSTM yang dibuat.

| ![Arsitektur BiLSTM](https://i.ibb.co/m04Sxdr/arsitektur-Bi-LSTM.png) |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|

Gambar di atas merupakan arsitektur dari model BiLSTM yang telah dibuat. Arsitektur pertama yaitu lapisan input yang berukuran 50 kolom. Input tersebut kemudian mengirimkan data pada lapisan ***Embedding*** dengan *vocabulary* maksimum berjumlah 2500 dan dimensi *embedding* berukuran 50. Hasil *embedding* kemudian akan dikirimkan pada lapisan ***SpatialDropout1D***. Setelah itu, data akan dikirimkan kepada lapisan BiLSTM pertama dengan jumlah unit sebesar 128 dan *activation function* ReLu. Lapisan berikutnya adalah lapisan BiLSTM kedua (*stacked*) dengan jumlah unit yang sama yaitu 128. Setelah data melalui lapisan BiLSTM, kemudian akan dikirim pada lapisan ***Dense*** (neuron biasa) dengan jumlah unit sebesar 64. Hasil dari lapisan ini kemudian akan dikirimkan pada lapisan terakhir (*output layer*) dengan total *output* sebesar 18.

###### 1.3 Pelatihan Model
Pada proses ini, data akan digunakan dan dipelajari representasinya oleh model yang telah dibuat. Pada tahap ini, digunakan dua (2) buah *subset* data yaitu data *train* dan data *validation*. Data *train* digunakan untuk pelatihan model sementara data *validation* digunakan untuk mengukur performa model pada data baru/belum pernah dilihat. Data *validation* didapatkan dengan memecah (*split*) data *train* dengan proporsi sebesar 20%. Berikut merupakan konfigurasi dari tahap pelatihan model.
| No. | Hyperparameter | Nilai                           |
|-----|----------------|---------------------------------|
| 1.  | Optimizer      | Adam                            |
| 2.  | Loss           | Sparse Categorical Crossentropy |
| 3.  | Epochs         | 100                             |
| 4.  | Batch Size     | 128                             |

Hasil dari proses pelatihan model akan dibahas pada tahap evaluasi.

##### 2. Testing
Pada tahap ini, model akan melakukan prediksi terhadap data uji (*testing*) yang belum pernah dilihat sebelumnya. Hal ini bertujuan untuk mengukur performa model untuk melakukan generalisasi. Data uji yang digunakan merupakan hasil *split* *dataset* dengan rasio 20% sehingga menghasilkan *subset* data dengan jumlah 397 observasi. Hasil *testing* akan dibahas pada tahap evaluasi.

</details>

---

<a id="bert"></a>
## **BERT** [ğŸ”](#daftar-isi)

<details>
<summary><b>Klik untuk Lihat Detailâ¤µï¸</b></summary>

### Penjelasan Algoritma BERT
Algoritma BERT atau *Bidirectional Encoder Representations from Transformers* adalah teknik berbasis jaringan saraf untuk *pre-training* Natural Language. BERT dirancang untuk membantu komputer memahami makna bahasa ambigu dalam teks dengan menggunakan teks di sekitarnya untuk membangun konteks. Kerangka kerja BERT telah dilatih sebelumnya menggunakan teks dari Wikipedia dan dapat disesuaikan dengan kumpulan data pertanyaan dan jawaban.

![GoogleBert_1920](https://user-images.githubusercontent.com/76557114/208561287-ad16340c-16e7-481a-a7fc-7ef75d94daab.jpg)

Model ini diusulkan dan diterbitkan oleh Jacob Devlin dan rekan-rekannya dari Google Research pada tahun 2018. Selanjutnya di tahun 2019, Google mengumumkan bahwa BERT mulai diimplementasikan di mesin pencarinya. Kemudian pada akhir tahun 2020, BERT digunakan di hampir semua kueri pencarian bahasa Inggris. BERT juga sudah bersifat open-source dari Google pada November 2018. Artinya, siapa pun dapat menggunakan BERT untuk melatih sistem pemrosesan bahasa mereka sendiri untuk menjawab pertanyaan atau tugas lain.


#### Cara Kerja Algoritma BERT
BERT memanfaatkan Transformer, mekanisme *attention* yang mempelajari hubungan kontekstual antara kata (atau sub-kata) dalam sebuah teks. Berbeda dengan model terarah, yang membaca input teks secara berurutan (kiri ke kanan atau kanan ke kiri), *encoder* Transformer membaca seluruh urutan kata sekaligus. Oleh sebab itu, BERT dianggap dua arah, meskipun akan lebih akurat untuk mengatakan bahwa itu tidak terarah. Karakteristik ini memungkinkan model mempelajari konteks sebuah kata berdasarkan seluruh lingkungannya (kiri dan kanan kata).

BERT juga merupakan teknik NLP pertama yang hanya mengandalkan mekanisme *self-attention*, yang dimungkinkan oleh *bidirectional* Transformer di pusat desain BERT. Ini penting karena sering kali sebuah kata dapat berubah artinya saat kalimat berkembang. Setiap kata yang ditambahkan menambah arti keseluruhan dari kata yang difokuskan oleh algoritma NLP. Semakin banyak kata yang hadir secara total dalam setiap kalimat atau frase, semakin ambigu kata yang menjadi fokus. BERT memperhitungkan makna yang diperbesar dengan membaca dua arah, memperhitungkan efek dari semua kata lain dalam sebuah kalimat pada kata fokus dan menghilangkan momentum kiri-ke-kanan yang mencondongkan kata-kata menuju makna tertentu saat kalimat berkembang.

![whatis-bert-h](https://user-images.githubusercontent.com/76557114/208569645-a6062700-7d13-4e1f-94e7-fa88ae368c26.png)

Misalnya, pada gambar di atas, BERT menentukan kata sebelumnya dalam kalimat yang dirujuk oleh kata "*is*", dan kemudian menggunakan mekanisme perhatiannya untuk menimbang opsi. Kata dengan skor terhitung tertinggi dianggap asosiasi yang benar (yaitu, "*is*" mengacu pada "*animal*", bukan "*he*"). Jika frasa ini adalah permintaan pencarian, hasilnya akan mencerminkan pemahaman yang lebih halus dan lebih tepat yang dicapai BERT ini.


#### Pemanfaatan BERT
BERT unggul dan dapat dimanfaatkan untuk beberapa tugas berikut.

###### Pembangkit bahasa berbasis *sequence-to-sequence*, seperti:
- *Question answering*
- *Abstract summarization*
- *Sentence prediction*
- *Conversational response generation*

###### Natural Language Understanding, seperti:
- *Polysemy and coreference resolution*
- *Word sense disambiguation*
- *Natural language inference*
- *Sentiment classification*

</details>

<a id="analisis-model-bert"></a>
### **Analisis Model BERT** [ğŸ”](#daftar-isi)

<details>
<summary><b>Klik untuk Lihat Detailâ¤µï¸</b></summary>
==masuk=ANALISIS-HASIL-EVALUASI-BERT
</details>

<a id="training-testing-bert"></a>
### **Training dan Testing BERT** [ğŸ”](#daftar-isi)

<details>
<summary><b>Klik untuk Lihat Detailâ¤µï¸</b></summary>

#### BERT
---
Proses pembuatan model BERT (*Bidirectional Encoder for Representation Transformers*) dilakukan dalam 3 tahap, yaitu *training*, *testing*, dan *evaluation*, Berikut merupakan penjelasan masing-masing tahap.
###### 1. Training
Pada tahap *training*, beberapa proses yang dilakukan antara lain **persiapan data latih**, **pembuatan arsitektur model**, **pelatihan model**.

###### 1.1 Persiapan Data Latih
Pada proses persiapan data latih, seluruh *dataset* yang digunakan akan dipecah (*split*) menjadi beberapa *subset*, diantaranya data *train* (latih) dan data *validation* (validasi). Data *train* dan data *validation* dibuat dengan melakukan *split* terhadap *dataset* dengan menggunakan fungsi `train_test_split` dari *library* sklearn. Seluruh *dataset* dipecah menjadi dua (2) buah *subset*, yaitu *data train* dan *data validation* dengan proporsi 80% dan 20%. Data *train* dan data *validation* kemudian digunakan pada tahap *training*.

###### 1.2 Pembuatan Arsitektur Model
Pada proses ini, dilakukan pembuatan arsitektur model BERT dengan menggunakan model *pre-trained* indoBERT yang berasal dari indoBenchmark. IndoBERT merupakan salah satu arsitektur *transformer pre-trained* yang dilatih pada data teks Bahasa Indonesia. Model indoBERT dapat diunduh pada *website* HuggingFace.

###### 1.3 Pelatihan Model
Pada proses ini, data akan digunakan dan dipelajari representasinya oleh model yang telah dibuat. Pada tahap ini, digunakan dua (2) buah *subset* data yaitu data *train* dan data *validation*. Data *train* digunakan untuk pelatihan model sementara data *validation* digunakan untuk mengukur performa model pada data baru/belum pernah dilihat. Data *train* dan data *validation* dilakukan tokenisasi terlebih dahulu menggunakan *tokenizer* bawaan indoBERT sehingga menghasilkan tiga (3) buah nilai baru, yaitu `attention_mask`, `input_ids`, `token_type_ids`. Ketiga nilai ini kemudian akan digunakan dalam proses pelatihan model. Berikut merupakan *hyperparameter* yang digunakan saat pelatihan model.
| No. | Hyperparameter | Nilai                           |
|-----|----------------|---------------------------------|
| 1.  | Optimizer      | Adam                            |
| 2.  | Loss           | Sparse Categorical Crossentropy |
| 3.  | Epochs         | 30                             |
| 4.  | Batch Size     | 16                             |

Hasil dari proses pelatihan model akan dibahas pada tahap evaluasi.

</details>

---

<a id="perbandingan-model"></a>
## **Perbandingan Masing-masing Model** [ğŸ”](#daftar-isi)

<details>
<summary><b>Klik untuk Lihat Detailâ¤µï¸</b></summary>


#### **Perbandingan**

Selanjutnya, hasil percobaan terbaik dibandingkan dengan beberapa model yang juga diimplementasikan. Tim kami mengimplementasikan beberapa model deep learning yang berbeda untuk membuat sebuah AI Chatbot, yaitu model LSTM, Bidirectional LSTM, dan BERT. Seluruh proses yang berkaitan dengan persiapan data dibuat sama persis untuk menjaga konsistensi data yang menjadi masukan model sekaligus untuk mendapatkan ukuran perbandingan antar model.

---

##### **Perbandingan Implementasi LSTM dengan BiLSTM**

Implementasi BiLSTM pada aplikasi kami menyerupai dengan implementasi LSTM. Letak perbedaan dari kedua implementasi ini terdapat pada lapisan LSTM pada arsitektur yang diganti dengan lapisan BiLSTM.

Implementasi arsitektur Bidirectional LSTM serupa dengan LSTM, namun lapisan LSTM yang ada di-wrap ke dalam lapisan Bidirectional RNN yang akan membuat dua lapisan LSTM dengan dua arah berbeda, satu ke arah forward dan satu ke arah backward. 

Lapisan Bidirectional ini memungkinkan setiap komponen urutan input memiliki informasi dari sekuens sebelum dan saat ini (past and present).Perbedaan lebih detail dapat dilihat pada gambar berikut.

![](https://www.researchgate.net/profile/Arvind-Mohan-3/publication/324769532/figure/fig2/AS:619510805561344@1524714294669/LSTM-and-BiLSTM-Architectures_W640.jpg)

Model BiLSTM dilatih dengan jumlah epoch sebanyak 100, dengan hyperparameter yang sama dengan model LSTM. Grafik nilai loss dan akurasi, serta nilai pada epoch terakhir dari model BiLSTM dapat dilihat pada kedua gambar berikut.

![](https://i.ibb.co/qnJTgBs/bilstm.jpg)

![](https://i.ibb.co/SmJKjm4/Screenshot-2022-12-19-at-17-30-56.png)

Untuk memudahkan perbandingan kedua model, dapat dengan melihat tabel perbandingan dari metrik loss dan akurasi berikut.

| Metrik              | Model LSTM | Model BiLSTM |
| :------------------ | ---------: | -----------: |
| Training Loss       |     0,1509 |   **0,0506** |
| Validation Loss     |     2,7392 |   **1,6496** |
| Training Accuracy   |     92,03% |   **97,59%** |
| Validation Accuracy |     69,75% |   **75,26%** |

Model BiLSTM yang kami kembangkan **menghasilkan performa yang lebih baik dibandingkan dengan model LSTM**. Akurasi maksimal yang didapatkan pada proses training mencapai 97,59% dan akurasi validasi mencapai 75,26%. Model LSTM hanya mampu mendapatkan akurasi 92,03% pada proses training, dan 69,75% pada validasi.

Nilai loss juga memiliki perbedaan signifikan, dimana model BiLSTM secara umum memiliki nilai loss yang lebih kecil dibanding model LSTM. Nilai loss minimal yang didapatkan pada proses training adalah 0,0506 dan nilai loss validasi adalah 1,6496. Model LSTM memiliki nilai loss 0,1509 pada proses training, dan 2,7392 pada proses validasi.

Model BiLSTM menghasilkan akurasi yang lebih tinggi karena menggunakan lapisan bidirectional yang memungkinkan terdapat dua LSTM pada satu lapisan, dengan dua arah yang berbeda. Hal ini memungkinkan data input untuk diproses dalam dua arah sekuens, sehingga menghasilkan representasi fitur kata dalam satu kalimat yang lebih baik, yang membantu meningkatkan performa model secara signifikan.

---

##### **Perbandingan Implementasi LSTM dengan BERT**

Model BERT yang diimplementasikan dalam percobaan ini adalah model **IndoBERT** `indobert-base-p2` yang didapat dari [huggingface](https://huggingface.co/indobenchmark/indobert-base-p2). Model diimplementasikan dengan AutoModel `TFAutoModelForSequenceClassification`, yang dapat membangun model klasifikasi menggunakan basis model lain.

Perbedaan lain dari implementasi model LSTM dengan BERT adalah fungsi loss yang digunakan, dimana implementasi LSTM menggunakan `categorical_crossentropy` sedangkan implementasi BERT menggunakan `sparse_categorical_crossentropy`. Hal ini karena label klasifikasi model BERT menggunakan label integer, sedangkan model LSTM menggunakan one-hot encoding. Model BERT dilatih dengan jumlah epoch sebanyak 30 dengan optimizer `adam`.

Grafik nilai loss dan akurasi, serta nilai pada epoch terakhir dari model BERT dapat dilihat pada kedua gambar berikut.

![](https://i.ibb.co/2MpB8Nr/bert.jpg)

![](https://i.ibb.co/563RRkZ/Screenshot-2022-12-20-at-20-27-01.png)

Untuk memudahkan perbandingan kedua model, dapat dengan melihat tabel perbandingan dari metrik loss dan akurasi berikut.

| Metrik              | Model LSTM | Model BERT |
| :------------------ | ---------: | ---------: |
| Training Loss       | **0,1509** |     2,5638 |
| Validation Loss     |     2,7392 | **2,6662** |
| Training Accuracy   | **92,03%** |     18,59% |
| Validation Accuracy | **69,75%** |     15,20% |

Model BERT yang kami kembangkan **menghasilkan performa yang lebih buruk dibandingkan dengan model LSTM**. Model BERT hanya mampu mendapatkan akurasi 18,59% pada proses training dan 15,20% pada validasi, sedangkan model LSTM mendapatkan akurasi maksimal 92,03% pada proses training dan 69,75% pada proses validasi.

Nilai loss juga memiliki perbedaan signifikan, dimana model LSTM secara umum memiliki nilai loss yang lebih kecil dibanding model BERT. Nilai loss minimal yang didapatkan pada proses training adalah 0,1509 dan nilai loss validasi adalah 2,7392. Model BERT memiliki nilai loss 2,5638 pada proses training, dan 2,6662 pada proses validasi.

Hasl ini menunjukkan bahwa model BERT yang diimplementasikan belum berhasil melakukan pembelajaran dengan baik terhadap dataset yang digunakan. Kami belum menemukan alasan spesifik mengenai hal ini, sehingga perlu penelitian lebih lanjut.

---

##### **Perbandingan Implementasi LSTM, BiLSTM, dan BERT**

Perbandingan terakhir digunakan untuk melihat model terbaik yang dapat digunakan pada aplikasi chatbot berdasarkan performa model yang telah diteliti, yang dapat dilihat pada tabel berikut.

| Metrik              | Model LSTM | Model BiLSTM | Model BERT |
| :------------------ | ---------: | -----------: | ---------: |
| Training Loss       |     0,1509 |   **0,0506** |     2,5638 |
| Validation Loss     |     2,7392 |   **1,6496** |     2,6662 |
| Training Accuracy   |     92,03% |   **97,59%** |     18,59% |
| Validation Accuracy |     69,75% |   **75,26%** |     15,20% |

Dari tabel perbandingan di atas, dapat dilihat bahwa model dengan performa terbaik secara keseluruhan adalah model BiLSTM. Model BiLSTM memiliki nilai akurasi tertinggi pada proses training sebesar 97,59% dan validasi sebesar 75,26%, serta nilai loss terendah pada proses training sebesar 0,0506 dan validasi sebesar 1,6495. Sehingga, model yang kami gunakan pada aplikasi chatbot adalah model BiLSTM.

</details>

---

<a id="alasan-model-unggul"></a>
## **Alasan Model yang Digunakan Paling Unggul** [ğŸ”](#daftar-isi)

<details>
<summary><b>Klik untuk Lihat Detailâ¤µï¸</b></summary>

Kami melakukan percobaan implementasi model LSTM, BiLSTM, dan BERT sebagai basis model deep learning AI Chatbot Teman Ngorte. Dari percobaan yang kami lakukan, didapatkan hasil bahwa model BiLSTM memiliki performa terbaik diantara seluruh model yang kami coba, dengan akurasi training sebesar 97,59% dan validasi sebesar 75,26%, serta nilai loss terendah pada proses training sebesar 0,0506 dan validasi sebesar 1,6495.

Dari performa tersebut, maka kami menggunakan model BiLSTM sebagai basis model AI Chatbot Teman Ngorte. 

</details>

---

<h2 align="center" width="100%">
    â€»â€»â€» <br>
    <a href="#daftar-isi">â˜ğŸ» <b>Kembali ke Daftar Isi</b></a> <br>
    â€»â€»â€»
</h2>

<!-- ## [â˜ğŸ» **Kembali ke Daftar Isi**](#daftar-isi) -->

---